{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd08479c18a240fe6377606edf314d34d4695e22aa3d07c9cc35ed68a172a35e609",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Linguistic measure collection\n",
    "\n",
    "著者の態度や心理的傾向との関係が示唆されている、著者検出のための文体測定法を集めたライブラリです。\n",
    "本ライブラリは、Asaishi(2017)で整理された日本語テキストメトリクスをベースに、12種類のスタイロメトリクスを算出することができる。\n",
    "\n",
    "対応する自然言語\n",
    "\n",
    "- 日本語\n",
    "- 英語\n",
    "\n",
    "## インストール\n",
    "\n",
    "> NOTE: `pip` によるインストール可能なパッケージは、現在開発中\n",
    "\n",
    "### 要件\n",
    "\n",
    "- Python 3.6 以上\n",
    "  - f-string (PEP 536):\n",
    "  - Type Hints (PEP 484):\n",
    "\n",
    "### 依存関係\n",
    "\n",
    "`pipenv` を使用している場合は、`pipenv install` でこのライブラリの仮想環境が作成されます。\n",
    "それ以外の場合は、`pip install -r requirements.txt` で十分です。\n",
    "\n",
    "また，本ライブラリは，日本語処理のために以下の外部コマンドにも依存しています．\n",
    "\n",
    "- MeCab (日本語テキスト処理用):\n",
    "    - [mecab-ipadic-neologd](https://github.com/neologd/mecab-ipadic-neologd) (日本語のNERを向上させる):\n",
    "- CaboCha (`analyse_parseddoc.py` 用):\n",
    "\n",
    "### リソース\n",
    "\n",
    "さらに、日本語メトリクスの場合は、以下の言語資源が必要です。\n",
    "\n",
    "- [AWD-J EX](http://sociocom.jp/~data/2019-AWD-J/)をファイル名`AWD-J_EX.txt`とする。\n",
    "- JIWCをファイル名 `2017-11-JIWC.csv`として\n",
    "- 日本語のストップワードをファイル名 `stopwords_jp.txt` として保存\n",
    "\n",
    "以上を`data/`フォルダに入れます。\n",
    "\n",
    "> 今後はオプションにしていきます。\n",
    "> また、リソースの置き場所も設定できるようにする予定です。\n",
    "\n",
    "## 使い方\n",
    "\n",
    "**重要：テキストの正規化はユーザーの責任です**。\n",
    "\n",
    "### `measure_lang.py`\n",
    "\n",
    "実行することで、テキストファイルからテーブルデータを生成することができます。\n",
    "\n",
    "```\n",
    "python measure_lang.py [csv/excel ファイルパス] [ターゲットカラム名]\n",
    "```\n",
    "\n",
    "`your_data.csv` を入力すると、入力ファイルと同じ場所に `your_data.measure.csv` が作成されます。\n",
    "\n",
    "適切な文数を計算するために、入力テキストは1行に1文のような形式にすることをお勧めします。\n",
    "\n",
    "また、以下のようにインポートして各メジャーを使用することもできます。\n",
    "\n",
    "```python\n",
    "import measure_lang as ml\n",
    "\n",
    "def your_fancy_func(string):\n",
    "    # ...\n",
    "    res = ml.calc_ttrs(string)\n",
    "    # ...\n",
    "```\n",
    "\n",
    "すべての関数は文字列を読みます（改行された長い文章でもOKです）。\n",
    "文字列のリストに適用したい場合には、リストの上で反復する必要があります。\n",
    "\n",
    "各メジャーの詳細な使用方法は、そのメジャーのdocstring（ワークインプログレス）に記載されています。\n",
    "\n",
    "### `analyse_parseddoc.py`\n",
    "\n",
    "(ここのドキュメントは作業中です)\n",
    "\n",
    "準備します。\n",
    "- 1行に1つの文、空白行なし\n",
    "- ドキュメントは1つの空白行で分割されます\n",
    "\n",
    "例を挙げると\n",
    "```python\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import split_sentence  # find a great library to split sentences\n",
    "\n",
    "col = \"column_name\"\n",
    "\n",
    "with open('formatted_text.txt', 'w') as f:\n",
    "    f.writelines(\n",
    "        [\n",
    "            re.sub(\"^$\", \"\", re.sub(\"\\n+\", \"\\n\", re.sub(\"\\s\", \" \", t))) + \"\\n\\n\"\n",
    "            for t in df[col].to_list()\n",
    "        ]\n",
    "    )\n",
    "```\n",
    "そして，`sentence-splitter formatted_text.txt > formatted_text-sentsplit.txt ` などと実行すると，`sentence-splitter` が入力されたテキストを，空白行を保持したまま一行一文の形式に分割します（良い本物のツールを見つけなければなりませんが，英語では `nltk` が提供しています）．\n",
    "\n",
    "実行してみてください。\n",
    "```shell\n",
    "cabocha -f1 your_text.txt > your_text.cabocha.txt\n",
    "python analyse_parseddoc.py your_text.cabocha.txt [your_csv.measured.csv]\n",
    "```\n",
    "\n",
    "第2引数に `measure_lang.py` で生成された測定済みの csv ファイルを追加すると、`analyse_parseddoc.py` は結果を列ごとに連結します (GNU `paste(1)` のように、ただし csv 内のエスケープされた複数行のテキストを考慮します)。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Measures\n",
    "\n",
    "```python\n",
    "import measure_lang as ml\n",
    "import analyse_parseddoc as ap\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- **Percentages of character types** (`ml.count_charcat`):  \n",
    "テキスト中の文字に対する、ひらがな、カタカナ、漢字のそれぞれの文字種の割合を表します。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_charcat(text: str) -> np.ndarray:\n",
    "    text = re.sub(r\"\\s\", \" \", text)\n",
    "    c = Counter([ud.name(char).split()[0] for char in text])\n",
    "    counts = np.array([c[\"HIRAGANA\"], c[\"KATAKANA\"], c[\"CJK\"]])\n",
    "    return np.divide(counts, len(text))"
   ]
  },
  {
   "source": [
    "- **Type Token Ratio (TTR)** (`ml.calc_ttrs`):  \n",
    "テキスト中の総単語数に対する異種単語の比率。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ttrs(tokens: List[Tuple[str, List[str]]]) -> np.ndarray:\n",
    "    cnt = Counter([token[1][6] for token in tokens])\n",
    "    Vn = len(cnt)\n",
    "    logVn = np.log(Vn)\n",
    "    N = np.sum(list(cnt.values()))\n",
    "    logN = np.log(N)\n",
    "    # TODO: implement frequency-wise TTR variants\n",
    "    return np.array(\n",
    "        [\n",
    "            np.divide(Vn, N),  # original plain TTR: not robust to the length\n",
    "            np.divide(Vn, np.sqrt(N)),  # Guiraud's R\n",
    "            np.divide(logVn, logN),  # Herdan's C_H\n",
    "            np.divide(logVn, np.log(logN)),  # Rubet's k\n",
    "            np.divide((logN - logVn), (logN ** 2)),  # Maas's a^2\n",
    "            np.divide((1 - (Vn ** 2)), ((Vn ** 2) * logN)),  # Tuldava's LN\n",
    "            np.float_power(N, np.float_power(Vn, 0.172)),  # Brunet's W\n",
    "            np.divide((logN ** 2), (logN - logVn)),  # Dugast's U\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "source": [
    "- **Percentage of content words** (`ml.measure_pos`):  \n",
    "テキストの総単語数に対する内容語(名詞、動詞、形容詞、副詞)の割合です。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- **Modifying words and Verb Ratio (MVR)** (`ml.measure_pos`):  \n",
    "文章中の単語に対して、動詞と形容詞・副詞・接続詞の割合を表したもの。作者推定の指標の一つとして用いられている。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- **Percentage of proper nouns** (`ml.measure_pos`):  \n",
    "テキスト中の全単語に対する固有名詞(名前付き実体)の割合です。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_pos(text: str) -> np.ndarray:\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NM.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    # print(tokens)\n",
    "\n",
    "    # VERB RELATED MEASURES\n",
    "    verbs = [token for token in tokens if token[1][0] == \"動詞\"]\n",
    "    # TODO: 助動詞との連語も含める？\n",
    "    # lens_verb = [len(verb) for verb in verbs]\n",
    "\n",
    "    # CONTENT WORDS RATIO\n",
    "    nouns = [token for token in tokens if token[1][0] == \"名詞\"]\n",
    "    adjcs = [token for token in tokens if token[1][0] == \"形容詞\"]\n",
    "    content_words = verbs + nouns + adjcs\n",
    "    cwr_simple = np.divide(len(content_words), len(tokens))\n",
    "    cwr_advance = np.divide(\n",
    "        len(\n",
    "            [\n",
    "                token\n",
    "                for token in content_words\n",
    "                if (token[1][1] not in STOPPOS_JP) and (token[0] not in STOPWORDS_JP)\n",
    "            ]\n",
    "        ),\n",
    "        len(tokens),\n",
    "    )\n",
    "\n",
    "    # NOTE: skip FUNCTION WORDS RATIO since it's equiv to 1 - CWR\n",
    "\n",
    "    # Modifying words and verb ratio (MVR)\n",
    "    advbs = [token for token in tokens if token[1][0] == \"副詞\"]\n",
    "    padjs = [token for token in tokens if token[1][0] == \"連体詞\"]\n",
    "    mvr = np.divide(len(adjcs + advbs + padjs), len(verbs))\n",
    "\n",
    "    # NER\n",
    "    ners = [token for token in tokens if token[1][1] == \"固有名詞\"]\n",
    "    nerr = np.divide(len(ners), len(tokens))\n",
    "\n",
    "    # TTR\n",
    "    ttrs = calc_ttrs(tokens)\n",
    "\n",
    "    return np.concatenate(\n",
    "        (\n",
    "            np.array(\n",
    "                [\n",
    "                    # np.mean(lens_verb),\n",
    "                    # np.std(lens_verb),\n",
    "                    # np.min(lens_verb),\n",
    "                    # np.quantile(lens_verb, 0.25),\n",
    "                    # np.median(lens_verb),\n",
    "                    # np.quantile(lens_verb, 0.75),\n",
    "                    # np.max(lens_verb),\n",
    "                    cwr_simple,\n",
    "                    cwr_advance,\n",
    "                    mvr,\n",
    "                    nerr,\n",
    "                ]\n",
    "            ),\n",
    "            ttrs,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "source": [
    "- **Word abstraction** (`ml.measure_abst`):  \n",
    "テキストに含まれる単語の抽象度。具体的には、最も抽象度の高い単語の最大値と、抽象度の高い上位5つの単語の平均値を用いました。抽象度は、日本語の単語抽象度辞書である [AWD-J EX](http://sociocom.jp/~data/2019-AWD-J/)から取得した。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_abst(text: str) -> np.ndarray:\n",
    "    # AWD uses neologd\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NMN.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    # print(tokens)\n",
    "    scores = [\n",
    "        float(AWD.get(token[0] if token[1][6] == \"*\" else token[1][6], 0))\n",
    "        for token in tokens\n",
    "    ]\n",
    "    # print(scores)\n",
    "\n",
    "    # top k=5 mean\n",
    "    return np.array([np.mean(sorted(scores, reverse=True)[:5]), max(scores)])\n"
   ]
  },
  {
   "source": [
    "- **Ratios of emotional words** (`ml.calc_jiwc`):  \n",
    "「悲しみ」、「不安」、「怒り」、「嫌悪」、「信頼」、「驚き」、「喜び」の7つの感情に関連する単語の、テキスト中の全単語に対する比率を表しています。7つの値は、確率の性質を満たすように変換されています（各値は0から1の間にあり、すべての値の合計は1になります）。感情との関連度は、日本語の感情語辞典JIWCに基づいて決定した。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_jiwc(text: str) -> np.ndarray:\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NM.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    jiwc_words = set(\n",
    "        [token[0] if token[1][6] == \"*\" else token[1][6] for token in tokens]\n",
    "    ) & set(DF_jiwc.index)\n",
    "    jiwc_vals = DF_jiwc.loc[jiwc_words].sum()\n",
    "    return np.divide(jiwc_vals, jiwc_vals.sum())\n",
    "    # Sad Anx Anger Hate Trustful S Happy\n"
   ]
  },
  {
   "source": [
    "- **Number of sentences** (`ml.measure_sents`):  \n",
    "テキストを構成する文の総数。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- **Length of sentences** (`ml.measure_sents`):  \n",
    "文章を構成する各文の文字数の記述統計（平均値、標準偏差、四分位値、最小値、最大値）。  \n",
    "特に、平均文量は、書き手の創造的な態度や性格との関連が示唆されている。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_sents(text: str) -> np.ndarray:\n",
    "    \"\"\"Show descriptive stats of sentence length.\n",
    "\n",
    "    input text should be one sentence per line.\n",
    "    \"\"\"\n",
    "    # sents = DELIM_SENT.split(text)\n",
    "    if \"\\r\" in text:\n",
    "        sents = text.split(\"\\r\\n\")\n",
    "    else:\n",
    "        sents = text.split(\"\\n\")\n",
    "    lens_char = np.array([len(sent) for sent in sents])\n",
    "    return np.array(\n",
    "        [\n",
    "            len(lens_char),\n",
    "            np.mean(lens_char),\n",
    "            np.std(lens_char, ddof=1),\n",
    "            np.min(lens_char),\n",
    "            np.quantile(lens_char, 0.25),\n",
    "            np.median(lens_char),\n",
    "            np.quantile(lens_char, 0.75),\n",
    "            np.max(lens_char),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "source": [
    "- **Percentage of conversational sentences** (`ml.count_conversations`):  \n",
    "テキストに含まれる会話文の総数の割合。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_conversations(text: str) -> float:\n",
    "    # 会話文の割合\n",
    "    text = re.sub(r\"\\s\", \" \", text)\n",
    "    singles = re.findall(r\"「.+?」\", text)\n",
    "    doubles = re.findall(r\"『.+?』\", text)\n",
    "    lens_single = [len(single) for single in singles]\n",
    "    lens_double = [len(double) for double in doubles]\n",
    "    return np.divide(sum(lens_single) + sum(lens_double), len(text))"
   ]
  },
  {
   "source": [
    "- **Depth of syntax tree** (`ap.analyse_dep`):  \n",
    "テキストの各文の依存関係ツリーの深さを算出した記述統計。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- **Mean of the number of chunks per sentence** (`ap.analyse_dep`):  \n",
    "文章ごとのチャンク数の平均値について計算された記述統計量。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- **Mean of the words per chunk** (`ap.analyse_dep`):  \n",
    "チャンクあたりの単語数の平均値を計算した記述統計量。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_dep(cfname: str, fname: str = None) -> None:\n",
    "    \"\"\"Apply dependency tree analyses and concat the original data\"\"\"\n",
    "    with open(cfname, \"r\") as f:\n",
    "        chunk_sents = read_chunks(f)\n",
    "\n",
    "    docs = []\n",
    "    doc = []\n",
    "    for chunk_sent in chunk_sents:\n",
    "        if chunk_sent:\n",
    "            doc.append(chunk_sent)\n",
    "        else:\n",
    "            docs.append(doc)\n",
    "            doc = []\n",
    "\n",
    "    sr_depths = []\n",
    "    sr_leaves = []\n",
    "    sr_chunklen = []\n",
    "    for doc in docs:\n",
    "        depths = [count_sent_depth(sentchunk) for sentchunk in doc]\n",
    "        sr_depths.append(pd.Series(depths).describe().to_frame().T)\n",
    "\n",
    "        n_leaves = [len(sentchunk) for sentchunk in doc]\n",
    "        sr_leaves.append(pd.Series(n_leaves).describe().to_frame().T)\n",
    "\n",
    "        chunklen = [len(chunk.morphs) for sentchunk in doc for chunk in sentchunk]\n",
    "        sr_chunklen.append(pd.Series(chunklen).describe().to_frame().T)\n",
    "\n",
    "    # 構文木の深さ\n",
    "    df_sdep = (\n",
    "        pd.concat(sr_depths)\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns=lambda x: f\"sdep_{x}\")\n",
    "    )\n",
    "    # 構文木の葉の数（文節数）\n",
    "    df_nleaf = (\n",
    "        pd.concat(sr_leaves)\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns=lambda x: f\"nleaf_{x}\")\n",
    "    )\n",
    "    # 文節の長さ（形態素数）\n",
    "    df_chklen = (\n",
    "        pd.concat(sr_chunklen)\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns=lambda x: f\"chklen_{x}\")\n",
    "    )\n",
    "\n",
    "    if fname:\n",
    "        df = pd.read_csv(fname)\n",
    "        assert len(df) == len(df_sdep)\n",
    "        pd.concat([df, df_sdep, df_nleaf, df_chklen], axis=1).to_csv(\n",
    "            f\"{fname}.parsed.csv\", index=False\n",
    "        )\n",
    "    pd.concat([df_sdep, df_nleaf, df_chklen], axis=1).to_csv(\n",
    "        f\"{cfname}.parsed.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "source": [
    "### 要約表\n",
    "\n",
    "| Stylometric                               | Sub-measures (value format)                                                 |\n",
    "| :---------------------------------------- | :-------------------------------------------------------------------------- |\n",
    "| Percentages of character types            | Hiragana, katakana, and kanji (Chinese characters) (%)                      |\n",
    "| Type Token Ration (TTR)                   | (%)                                                                         |\n",
    "| Percentages of content words              | (%)                                                                         |\n",
    "| Modifying words and Verb Ratio (MVR)      | (%)                                                                         |\n",
    "| Percentage of proper nouns                | (%)                                                                         |\n",
    "| Word abstraction                          | The maximum, and the average of the top five abstract words (real number)   |\n",
    "| Ratios of emotional words                 | sadness, anxiety, anger, disgust, trust, surprise, and happiness (%)        |\n",
    "| Number of sentences                       | (integer)                                                                   |\n",
    "| Length of sentences                       | mean, standard deviation, interquartile, minimum, and maximum (real number) |\n",
    "| Percentage of conversational sentences    | (%)                                                                         |\n",
    "| Depth of syntax tree                     | mean, standard deviation, interquartile, minimum, and maximum (real number) |\n",
    "| Mean of the number of chunks per sentence | mean, standard deviation, interquartile, minimum, and maximum (real number) |\n",
    "| Mean of the words per chunk               | mean, standard deviation, interquartile, minimum, and maximum (real number) |\n",
    "\n",
    "---\n",
    "## リファレンス\n",
    "\n",
    "- [Asaishi, 2007]: 浅石卓真. (2017). テキストの特徴を計量する指標の概観. 日本図書館情報学会誌, 63(3), 159–169. https://doi.org/10.20651/jslis.63.3_159"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Union\n",
    "import unicodedata as ud\n",
    "\n",
    "import fire\n",
    "from natto import MeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Num = Union[int, float]\n",
    "\n",
    "# TODO: 効率化\n",
    "# 前処理としてmecabのトークン化を行い、ライブラリでの利用を可能にする\n",
    "\n",
    "# TODO: これらのリソースをユーザーが割り当てられるようにする\n",
    "# TODO: これらのリソースをオプションにする\n",
    "# DELIM_SENT = re.compile(r\"(?:[。？！\\?\\!]+|[。？！\\?\\!]?[」』])\")  # FIXME: 文じゃないカギカッコが取れちゃう\n",
    "\n",
    "#この辺を引数にする Fire絡み　Apply file\n",
    "NM = MeCab()  # NOTE: assume IPADIC\n",
    "NMN = MeCab(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "with open(os.path.join(os.path.dirname(__file__), \"./data/stopwords_jp.txt\"), \"r\") as f:\n",
    "    STOPWORDS_JP = [line.strip() for line in f]\n",
    "STOPPOS_JP = [\"形容動詞語幹\", \"副詞可能\", \"代名詞\", \"ナイ形容詞語幹\", \"特殊\", \"数\", \"接尾\", \"非自立\"]\n",
    "\n",
    "with open(os.path.expanduser(\"./data/AWD-J_EX.txt\"), \"r\") as f:\n",
    "    rows = [line.strip().split(\"\\t\") for line in f]\n",
    "    AWD = {word: score for word, score, _, _ in rows}\n",
    "DF_jiwc = pd.read_csv(os.path.expanduser(\"./data/2017-11-JIWC.csv\"), index_col=1).drop(\n",
    "    columns=\"Unnamed: 0\"\n",
    ")\n",
    "\n",
    "\n",
    "def measure_sents(text: str) -> np.ndarray:\n",
    "    \"\"\"Show descriptive stats of sentence length.\n",
    "\n",
    "    input text should be one sentence per line.\n",
    "    \"\"\"\n",
    "    # sents = DELIM_SENT.split(text)\n",
    "    if \"\\r\" in text:\n",
    "        sents = text.split(\"\\r\\n\")\n",
    "    else:\n",
    "        sents = text.split(\"\\n\")\n",
    "    lens_char = np.array([len(sent) for sent in sents])\n",
    "    return np.array(\n",
    "        [\n",
    "            len(lens_char),\n",
    "            np.mean(lens_char),\n",
    "            np.std(lens_char, ddof=1),\n",
    "            np.min(lens_char),\n",
    "            np.quantile(lens_char, 0.25),\n",
    "            np.median(lens_char),\n",
    "            np.quantile(lens_char, 0.75),\n",
    "            np.max(lens_char),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def count_conversations(text: str) -> float:\n",
    "    # 会話文の割合\n",
    "    text = re.sub(r\"\\s\", \" \", text)\n",
    "    singles = re.findall(r\"「.+?」\", text)\n",
    "    doubles = re.findall(r\"『.+?』\", text)\n",
    "    lens_single = [len(single) for single in singles]\n",
    "    lens_double = [len(double) for double in doubles]\n",
    "    return np.divide(sum(lens_single) + sum(lens_double), len(text))\n",
    "\n",
    "\n",
    "def count_charcat(text: str) -> np.ndarray:\n",
    "    text = re.sub(r\"\\s\", \" \", text)\n",
    "    c = Counter([ud.name(char).split()[0] for char in text])\n",
    "    counts = np.array([c[\"HIRAGANA\"], c[\"KATAKANA\"], c[\"CJK\"]])\n",
    "    return np.divide(counts, len(text))\n",
    "\n",
    "\n",
    "def measure_pos(text: str) -> np.ndarray:\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NM.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    #この辺\n",
    "    # print(tokens)\n",
    "\n",
    "    # VERB RELATED MEASURES\n",
    "    verbs = [token for token in tokens if token[1][0] == \"動詞\"]\n",
    "    # TODO: 助動詞との連語も含める？\n",
    "    # lens_verb = [len(verb) for verb in verbs]\n",
    "\n",
    "    # CONTENT WORDS RATIO\n",
    "    nouns = [token for token in tokens if token[1][0] == \"名詞\"]\n",
    "    adjcs = [token for token in tokens if token[1][0] == \"形容詞\"]\n",
    "    content_words = verbs + nouns + adjcs\n",
    "    cwr_simple = np.divide(len(content_words), len(tokens))\n",
    "    cwr_advance = np.divide(\n",
    "        len(\n",
    "            [\n",
    "                token\n",
    "                for token in content_words\n",
    "                if (token[1][1] not in STOPPOS_JP) and (token[0] not in STOPWORDS_JP)\n",
    "            ]\n",
    "        ),\n",
    "        len(tokens),\n",
    "    )\n",
    "\n",
    "    # NOTE: skip FUNCTION WORDS RATIO since it's equiv to 1 - CWR\n",
    "\n",
    "    # Modifying words and verb ratio (MVR)\n",
    "    advbs = [token for token in tokens if token[1][0] == \"副詞\"]\n",
    "    padjs = [token for token in tokens if token[1][0] == \"連体詞\"]\n",
    "    mvr = np.divide(len(adjcs + advbs + padjs), len(verbs))\n",
    "\n",
    "    # NER\n",
    "    ners = [token for token in tokens if token[1][1] == \"固有名詞\"]\n",
    "    nerr = np.divide(len(ners), len(tokens))\n",
    "\n",
    "    # TTR\n",
    "    ttrs = calc_ttrs(tokens)\n",
    "\n",
    "    return np.concatenate(\n",
    "        (\n",
    "            np.array(\n",
    "                [\n",
    "                    # np.mean(lens_verb),\n",
    "                    # np.std(lens_verb),\n",
    "                    # np.min(lens_verb),\n",
    "                    # np.quantile(lens_verb, 0.25),\n",
    "                    # np.median(lens_verb),\n",
    "                    # np.quantile(lens_verb, 0.75),\n",
    "                    # np.max(lens_verb),\n",
    "                    cwr_simple,\n",
    "                    cwr_advance,\n",
    "                    mvr,\n",
    "                    nerr,\n",
    "                ]\n",
    "            ),\n",
    "            ttrs,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def measure_abst(text: str) -> np.ndarray:\n",
    "    # AWD uses neologd\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NMN.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    # print(tokens)\n",
    "    scores = [\n",
    "        float(AWD.get(token[0] if token[1][6] == \"*\" else token[1][6], 0))\n",
    "        for token in tokens\n",
    "    ]\n",
    "    # print(scores)\n",
    "\n",
    "    # top k=5 mean\n",
    "    return np.array([np.mean(sorted(scores, reverse=True)[:5]), max(scores)])\n",
    "\n",
    "\n",
    "def detect_bunmatsu(text: str) -> float:\n",
    "    if \"\\r\" in text:\n",
    "        sents = text.split(\"\\r\\n\")\n",
    "    else:\n",
    "        sents = text.split(\"\\n\")\n",
    "\n",
    "    # 体言止め\n",
    "    taigen = 0\n",
    "    for sent in sents:\n",
    "        tokens = [\n",
    "            (n.surface, n.feature.split(\",\"))\n",
    "            for n in NM.parse(text, as_nodes=True)\n",
    "            if not n.is_eos()\n",
    "        ]\n",
    "        taigen += 1 if tokens[-2][1] == \"名詞\" else 0\n",
    "    ratio_taigen = np.divide(taigen, len(sents))\n",
    "\n",
    "    # TODO: what else?\n",
    "\n",
    "    return ratio_taigen\n",
    "\n",
    "\n",
    "def calc_ttrs(tokens: List[Tuple[str, List[str]]]) -> np.ndarray:\n",
    "    cnt = Counter([token[1][6] for token in tokens])\n",
    "    Vn = len(cnt)\n",
    "    logVn = np.log(Vn)\n",
    "    N = np.sum(list(cnt.values()))\n",
    "    logN = np.log(N)\n",
    "    # TODO: implement frequency-wise TTR variants\n",
    "    return np.array(\n",
    "        [\n",
    "            np.divide(Vn, N),  # original plain TTR: not robust to the length\n",
    "            np.divide(Vn, np.sqrt(N)),  # Guiraud's R\n",
    "            np.divide(logVn, logN),  # Herdan's C_H\n",
    "            np.divide(logVn, np.log(logN)),  # Rubet's k\n",
    "            np.divide((logN - logVn), (logN ** 2)),  # Maas's a^2\n",
    "            np.divide((1 - (Vn ** 2)), ((Vn ** 2) * logN)),  # Tuldava's LN\n",
    "            np.float_power(N, np.float_power(Vn, 0.172)),  # Brunet's W\n",
    "            np.divide((logN ** 2), (logN - logVn)),  # Dugast's U\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_potentialvocab(text: str) -> float:\n",
    "    # 荒牧先生の潜在語彙量も\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def calc_jiwc(text: str) -> np.ndarray:\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NM.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    jiwc_words = set(\n",
    "        [token[0] if token[1][6] == \"*\" else token[1][6] for token in tokens]\n",
    "    ) & set(DF_jiwc.index)\n",
    "    jiwc_vals = DF_jiwc.loc[jiwc_words].sum()\n",
    "    return np.divide(jiwc_vals, jiwc_vals.sum())\n",
    "    # Sad Anx Anger Hate Trustful S Happy\n",
    "\n",
    "\n",
    "def apply_all(text: str) -> Dict[str, Num]:\n",
    "    try:\n",
    "        all_res = np.concatenate(\n",
    "            (\n",
    "                measure_sents(text),\n",
    "                [count_conversations(text)],\n",
    "                count_charcat(text),\n",
    "                measure_pos(text),\n",
    "                measure_abst(text),\n",
    "                [detect_bunmatsu(text)],\n",
    "                calc_jiwc(text),\n",
    "            )\n",
    "        )\n",
    "    except:\n",
    "        print(text)\n",
    "        raise\n",
    "    headers = [\n",
    "        \"num_sent\",\n",
    "        \"mean_sent_len\",\n",
    "        \"std_sent_len\",\n",
    "        \"min_sent_len\",\n",
    "        \"q1_sent_len\",\n",
    "        \"median_sent_len\",\n",
    "        \"q3_sent_len\",\n",
    "        \"max_sent_len\",\n",
    "        \"num_conv\",\n",
    "        \"pct_hiragana\",\n",
    "        \"pct_katakana\",\n",
    "        \"pct_kanji\",\n",
    "        \"cwr_simple\",\n",
    "        \"cwr_advance\",\n",
    "        \"mvr\",\n",
    "        \"pct_ner\",\n",
    "        \"ttr_plain\",\n",
    "        \"guiraud_r\",\n",
    "        \"herdan_ch\",\n",
    "        \"rubet_k\",\n",
    "        \"maas_a2\",\n",
    "        \"tuldava_ln\",\n",
    "        \"brunet_w\",\n",
    "        \"dugast_u\",\n",
    "        \"top5_mean_abst\",\n",
    "        \"max_abst\",\n",
    "        \"ratio_taigendome\",\n",
    "        \"jiwc_sadness\",\n",
    "        \"jiwc_anxiety\",\n",
    "        \"jiwc_anger\",\n",
    "        \"jiwc_hatrid\",\n",
    "        \"jiwc_trust\",\n",
    "        \"jiwc_surprise\",\n",
    "        \"jiwc_happiness\",\n",
    "    ]\n",
    "    return dict(zip(headers, all_res))\n",
    "\n",
    "\n",
    "def apply_file(fname, col):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        df = pd.read_csv(fname)\n",
    "    elif fname.endswith(\".xls\") or fname.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(fname)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input format: please use CSV or Excel data\")\n",
    "\n",
    "    assert col in df.columns, f\"{col} is not found in the input data\"\n",
    "\n",
    "    pd.concat(\n",
    "        [df, df.apply(lambda row: apply_all(row[col]), result_type=\"expand\", axis=1)],\n",
    "        axis=1,\n",
    "    ).to_csv(f\"{fname}.measured.csv\", index=False)\n",
    "#この辺\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(apply_file)\n"
   ]
  }
 ]
}