{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('limco': conda)"
  },
  "interpreter": {
   "hash": "144b6df7c7ca575248860d73cec7f80cd06c55c69386e9891e88fbc11a96701a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# tokenについてmecabとspacy/ginzaの比較"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Union\n",
    "import unicodedata as ud\n",
    "\n",
    "import fire\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "Num = Union[int, float]\n",
    "STOPPOS_JP = [\"形容動詞語幹\", \"副詞可能\", \"代名詞\", \"ナイ形容詞語幹\", \"特殊\", \"数\", \"接尾\", \"非自立\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open(os.path.expanduser(\"/Users/labimac/Documents/lab/ra_provisional/provisional_limco/stopwords_jp.txt\"), \"r\") as f:\n",
    "    STOPWORDS_JP = [line.strip() for line in f]\n",
    "STOPPOS_JP = [\"形容動詞語幹\", \"副詞可能\", \"代名詞\", \"ナイ形容詞語幹\", \"特殊\", \"数\", \"接尾\", \"非自立\"]\n",
    "\n",
    "with open(os.path.expanduser(\"/Users/labimac/Documents/lab/ra_provisional/provisional_limco/AWD-J_EX.txt\"), \"r\") as f:\n",
    "    rows = [line.strip().split(\"\\t\") for line in f]\n",
    "    AWD = {word: score for word, score, _, _ in rows}\n",
    "DF_jiwc = pd.read_csv(os.path.expanduser(\"/Users/labimac/Documents/lab/ra_provisional/provisional_limco/2017-11-JIWC.csv\"), index_col=1).drop(\n",
    "    columns=\"Unnamed: 0\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from natto import MeCab\n",
    "NM = MeCab()  # NOTE: assume IPADIC\n",
    "NMN = MeCab(\"-d /Users/labimac/Documents/lab/ra_provisional/provisional_limco/mecab-ipadic-neologd/build/mecab-ipadic-2.7.0-20070801-neologd-20200910\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import spacy\n",
    "import ginza\n",
    "NLP = spacy.load('ja_ginza')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "text = 'ソニーのプログラミング言語とは異なり、自然言語には言葉の曖昧性が存在します。'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def tokens(text: str) -> np.ndarray:\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NM.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tokens(text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('吾輩', ['名詞', '代名詞', '一般', '*', '*', '*', '吾輩', 'ワガハイ', 'ワガハイ']),\n",
       " ('は', ['助詞', '係助詞', '*', '*', '*', '*', 'は', 'ハ', 'ワ']),\n",
       " ('猫', ['名詞', '一般', '*', '*', '*', '*', '猫', 'ネコ', 'ネコ']),\n",
       " ('で', ['助動詞', '*', '*', '*', '特殊・ダ', '連用形', 'だ', 'デ', 'デ']),\n",
       " ('ある', ['助動詞', '*', '*', '*', '五段・ラ行アル', '基本形', 'ある', 'アル', 'アル']),\n",
       " ('。', ['記号', '句点', '*', '*', '*', '*', '。', '。', '。']),\n",
       " ('名前', ['名詞', '一般', '*', '*', '*', '*', '名前', 'ナマエ', 'ナマエ']),\n",
       " ('は', ['助詞', '係助詞', '*', '*', '*', '*', 'は', 'ハ', 'ワ']),\n",
       " ('まだ', ['副詞', '助詞類接続', '*', '*', '*', '*', 'まだ', 'マダ', 'マダ']),\n",
       " ('無い', ['形容詞', '自立', '*', '*', '形容詞・アウオ段', '基本形', '無い', 'ナイ', 'ナイ']),\n",
       " ('。', ['記号', '句点', '*', '*', '*', '*', '。', '。', '。'])]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "def tokens_ginza(text: str) -> np.ndarray:\n",
    "    text = text.replace('一\\n\\n　', '')\n",
    "    doc = NLP(text)\n",
    "    tokens = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            token_tag = re.split('[,-]', token.tag_)\n",
    "            token_inflection = re.split('[,-]', ginza.inflection(token))\n",
    "            analysis = token_tag + token_inflection\n",
    "            analysis.append(token.lemma_)\n",
    "            analysis += re.split('[,-]', ginza.reading_form(token))\n",
    "            tuple_ = (token.orth_, analysis)\n",
    "            tokens.append(tuple_)\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "tokens_ginza(text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('吾輩', ['代名詞', '', '吾輩', 'ワガハイ']),\n",
       " ('は', ['助詞', '係助詞', '', 'は', 'ハ']),\n",
       " ('猫', ['名詞', '普通名詞', '一般', '', '猫', 'ネコ']),\n",
       " ('で', ['助動詞', '助動詞', 'ダ', '連用形', '一般', 'だ', 'デ']),\n",
       " ('ある', ['動詞', '非自立可能', '五段', 'ラ行', '終止形', '一般', 'ある', 'アル']),\n",
       " ('。', ['補助記号', '句点', '', '。', '。']),\n",
       " ('名前', ['名詞', '普通名詞', '一般', '', '名前', 'ナマエ']),\n",
       " ('は', ['助詞', '係助詞', '', 'は', 'ハ']),\n",
       " ('まだ', ['副詞', '', 'まだ', 'マダ']),\n",
       " ('無い', ['形容詞', '非自立可能', '形容詞', '終止形', '一般', '無い', 'ナイ']),\n",
       " ('。', ['補助記号', '句点', '', '。', '。'])]"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def calc_ttrs(tokens: List[Tuple[str, List[str]]]) -> np.ndarray:\n",
    "    cnt = Counter([token[1][6] for token in tokens])\n",
    "    Vn = len(cnt)\n",
    "    logVn = np.log(Vn)\n",
    "    N = np.sum(list(cnt.values()))\n",
    "    logN = np.log(N)\n",
    "    return np.array(\n",
    "        [\n",
    "            np.divide(Vn, N),  # original plain TTR: not robust to the length\n",
    "            np.divide(Vn, np.sqrt(N)),  # Guiraud's R\n",
    "            np.divide(logVn, logN),  # Herdan's C_H\n",
    "            np.divide(logVn, np.log(logN)),  # Rubet's k\n",
    "            np.divide((logN - logVn), (logN ** 2)),  # Maas's a^2\n",
    "            np.divide((1 - (Vn ** 2)), ((Vn ** 2) * logN)),  # Tuldava's LN\n",
    "            np.float_power(N, np.float_power(Vn, 0.172)),  # Brunet's W\n",
    "            np.divide((logN ** 2), (logN - logVn)),  # Dugast's U\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def measure_pos(text: str) -> np.ndarray:\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NM.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    # print(tokens)\n",
    "\n",
    "    # VERB RELATED MEASURES\n",
    "    verbs = [token for token in tokens if token[1][0] == \"動詞\"]\n",
    "    # TODO: 助動詞との連語も含める？\n",
    "    # lens_verb = [len(verb) for verb in verbs]\n",
    "\n",
    "    # CONTENT WORDS RATIO\n",
    "    nouns = [token for token in tokens if token[1][0] == \"名詞\"]\n",
    "    adjcs = [token for token in tokens if token[1][0] == \"形容詞\"]\n",
    "    content_words = verbs + nouns + adjcs\n",
    "    cwr_simple = np.divide(len(content_words), len(tokens))\n",
    "    cwr_advance = np.divide(\n",
    "        len(\n",
    "            [\n",
    "                token\n",
    "                for token in content_words\n",
    "                if (token[1][1] not in STOPPOS_JP) and (token[0] not in STOPWORDS_JP)\n",
    "            ]\n",
    "        ),\n",
    "        len(tokens),\n",
    "    )\n",
    "\n",
    "    # Modifying words and verb ratio (MVR)\n",
    "    advbs = [token for token in tokens if token[1][0] == \"副詞\"]\n",
    "    padjs = [token for token in tokens if token[1][0] == \"連体詞\"]\n",
    "    mvr = np.divide(len(adjcs + advbs + padjs), len(verbs))\n",
    "\n",
    "    # NER\n",
    "    ners = [token for token in tokens if token[1][1] == \"固有名詞\"]\n",
    "    nerr = np.divide(len(ners), len(tokens))\n",
    "\n",
    "    # TTR\n",
    "    ttrs = calc_ttrs(tokens)\n",
    "\n",
    "    return np.concatenate(\n",
    "        (\n",
    "            np.array(\n",
    "                [\n",
    "                    # np.mean(lens_verb),\n",
    "                    # np.std(lens_verb),\n",
    "                    # np.min(lens_verb),\n",
    "                    # np.quantile(lens_verb, 0.25),\n",
    "                    # np.median(lens_verb),\n",
    "                    # np.quantile(lens_verb, 0.75),\n",
    "                    # np.max(lens_verb),\n",
    "                    cwr_simple,\n",
    "                    cwr_advance,\n",
    "                    mvr,\n",
    "                    nerr,\n",
    "                ]\n",
    "            ),\n",
    "            ttrs,\n",
    "        )\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "def calc_ttrs(tokens: List[Tuple[str, List[str]]]) -> np.ndarray:\n",
    "    doc = NLP(text.replace('一\\n\\n　', ''))\n",
    "    cnt = Counter([token.lemma_ for sent in doc.sents for token in sent])\n",
    "    Vn = len(cnt)\n",
    "    logVn = np.log(Vn)\n",
    "    N = np.sum(list(cnt.values()))\n",
    "    logN = np.log(N)\n",
    "    return np.array(\n",
    "        [\n",
    "            np.divide(Vn, N),  # original plain TTR: not robust to the length\n",
    "            np.divide(Vn, np.sqrt(N)),  # Guiraud's R\n",
    "            np.divide(logVn, logN),  # Herdan's C_H\n",
    "            np.divide(logVn, np.log(logN)),  # Rubet's k\n",
    "            np.divide((logN - logVn), (logN ** 2)),  # Maas's a^2\n",
    "            np.divide((1 - (Vn ** 2)), ((Vn ** 2) * logN)),  # Tuldava's LN\n",
    "            np.float_power(N, np.float_power(Vn, 0.172)),  # Brunet's W\n",
    "            np.divide((logN ** 2), (logN - logVn)),  # Dugast's U\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def measure_pos_ginza(text: str, stopwords) -> np.ndarray:\n",
    "    doc = NLP(text.replace('一\\n\\n　', ''))\n",
    "    tokens = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            token_tag = re.split('[,-]', token.tag_) # 品詞詳細\n",
    "            token_inflection = re.split('[,-]', ginza.inflection(token))  # 活用情報\n",
    "            analysis = token_tag + token_inflection\n",
    "            analysis.append(token.lemma_) # 基本形\n",
    "            tuple_ = (token.lemma_, analysis)\n",
    "            tokens.append(tuple_)\n",
    "    \n",
    "    verbs = [token for token in tokens if token[1][0] == \"動詞\"]\n",
    "    nouns = [token for token in tokens if token[1][0] == \"名詞\"]\n",
    "    adjcs = [token for token in tokens if token[1][0] == \"形容詞\"]\n",
    "    content_words = verbs + nouns + adjcs\n",
    "    cwr_simple = np.divide(len(content_words), len(tokens))\n",
    "    cwr_advance = np.divide(\n",
    "        len(\n",
    "            [\n",
    "                token\n",
    "                for token in content_words\n",
    "                if (token[1][1] not in STOPPOS_JP) and (token[0] not in stopwords)\n",
    "            ]\n",
    "        ),\n",
    "        len(tokens),\n",
    "    )\n",
    "\n",
    "    advbs = [token for token in tokens if token[1][0] == \"副詞\"]\n",
    "    padjs = [token for token in tokens if token[1][0] == \"連体詞\"]\n",
    "    mvr = np.divide(len(adjcs + advbs + padjs), len(verbs))\n",
    "    ners = [token for token in tokens if token[1][1] == \"固有名詞\"]\n",
    "    nerr = np.divide(len(ners), len(tokens))\n",
    "\n",
    "    ttrs = calc_ttrs(tokens)\n",
    "\n",
    "    return np.concatenate(\n",
    "        (\n",
    "            np.array(\n",
    "                [\n",
    "                    # np.mean(lens_verb),\n",
    "                    # np.std(lens_verb),\n",
    "                    # np.min(lens_verb),\n",
    "                    # np.quantile(lens_verb, 0.25),\n",
    "                    # np.median(lens_verb),\n",
    "                    # np.quantile(lens_verb, 0.75),\n",
    "                    # np.max(lens_verb),\n",
    "                    cwr_simple,\n",
    "                    cwr_advance,\n",
    "                    mvr,\n",
    "                    nerr,\n",
    "                ]\n",
    "            ),\n",
    "            ttrs,\n",
    "        )\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "measure_pos(text)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "array([ 5.26315789e-01,  3.68421053e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        8.94736842e-01,  3.90006748e+00,  9.62225186e-01,  2.62354416e+00,\n",
       "        1.28292060e-02, -3.38448105e-01,  1.20705895e+02,  7.79471467e+01])"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "measure_pos_ginza(text, STOPWORDS_JP)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([ 5.00000000e-01,  4.00000000e-01,  0.00000000e+00,  5.00000000e-02,\n",
       "         8.50000000e-01,  3.80131556e+00,  9.45749849e-01,  2.58224802e+00,\n",
       "         1.81091454e-02, -3.32653155e-01,  1.31217880e+02,  5.52207173e+01]),\n",
       " [('ソニー', ['名詞', '固有名詞', '一般', '', 'ソニー']),\n",
       "  ('の', ['助詞', '格助詞', '', 'の']),\n",
       "  ('プログラミング', ['名詞', '普通名詞', 'サ変可能', '', 'プログラミング']),\n",
       "  ('言語', ['名詞', '普通名詞', '一般', '', '言語']),\n",
       "  ('と', ['助詞', '格助詞', '', 'と']),\n",
       "  ('は', ['助詞', '係助詞', '', 'は']),\n",
       "  ('異なる', ['動詞', '一般', '五段', 'ラ行', '連用形', '一般', '異なる']),\n",
       "  ('、', ['補助記号', '読点', '', '、']),\n",
       "  ('自然', ['名詞', '普通名詞', '一般', '', '自然']),\n",
       "  ('言語', ['名詞', '普通名詞', '一般', '', '言語']),\n",
       "  ('に', ['助詞', '格助詞', '', 'に']),\n",
       "  ('は', ['助詞', '係助詞', '', 'は']),\n",
       "  ('言葉', ['名詞', '普通名詞', '一般', '', '言葉']),\n",
       "  ('の', ['助詞', '格助詞', '', 'の']),\n",
       "  ('曖昧性', ['名詞', '普通名詞', '一般', '', '曖昧性']),\n",
       "  ('が', ['助詞', '格助詞', '', 'が']),\n",
       "  ('存在', ['名詞', '普通名詞', 'サ変可能', '', '存在']),\n",
       "  ('する', ['動詞', '非自立可能', 'サ行変格', '連用形', '一般', 'する']),\n",
       "  ('ます', ['助動詞', '助動詞', 'マス', '終止形', '一般', 'ます']),\n",
       "  ('。', ['補助記号', '句点', '', '。'])])"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def calc_jiwc(text: str, df_jiwc) -> np.ndarray:\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NM.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    jiwc_words = set(\n",
    "        [token[0] if token[1][6] == \"*\" else token[1][6] for token in tokens]\n",
    "    ) & set(df_jiwc.index)\n",
    "    jiwc_vals = df_jiwc.loc[jiwc_words].sum()\n",
    "    return tokens, np.divide(jiwc_vals, jiwc_vals.sum())\n",
    "    # Sad Anx Anger Hate Trustful S Happy\n",
    "    \n",
    "print(calc_jiwc(text, DF_jiwc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "([('プログラミング', ['名詞', 'サ変接続', '*', '*', '*', '*', 'プログラミング', 'プログラミング', 'プログラミング']), ('言語', ['名詞', '一般', '*', '*', '*', '*', '言語', 'ゲンゴ', 'ゲンゴ']), ('と', ['助詞', '格助詞', '一般', '*', '*', '*', 'と', 'ト', 'ト']), ('は', ['助詞', '係助詞', '*', '*', '*', '*', 'は', 'ハ', 'ワ']), ('異なり', ['動詞', '自立', '*', '*', '五段・ラ行', '連用形', '異なる', 'コトナリ', 'コトナリ']), ('、', ['記号', '読点', '*', '*', '*', '*', '、', '、', '、']), ('自然', ['名詞', '形容動詞語幹', '*', '*', '*', '*', '自然', 'シゼン', 'シゼン']), ('言語', ['名詞', '一般', '*', '*', '*', '*', '言語', 'ゲンゴ', 'ゲンゴ']), ('に', ['助詞', '格助詞', '一般', '*', '*', '*', 'に', 'ニ', 'ニ']), ('は', ['助詞', '係助詞', '*', '*', '*', '*', 'は', 'ハ', 'ワ']), ('言葉', ['名詞', '一般', '*', '*', '*', '*', '言葉', 'コトバ', 'コトバ']), ('の', ['助詞', '連体化', '*', '*', '*', '*', 'の', 'ノ', 'ノ']), ('曖昧', ['名詞', '形容動詞語幹', '*', '*', '*', '*', '曖昧', 'アイマイ', 'アイマイ']), ('性', ['名詞', '接尾', '一般', '*', '*', '*', '性', 'セイ', 'セイ']), ('が', ['助詞', '格助詞', '一般', '*', '*', '*', 'が', 'ガ', 'ガ']), ('存在', ['名詞', 'サ変接続', '*', '*', '*', '*', '存在', 'ソンザイ', 'ソンザイ']), ('し', ['動詞', '自立', '*', '*', 'サ変・スル', '連用形', 'する', 'シ', 'シ']), ('ます', ['助動詞', '*', '*', '*', '特殊・マス', '基本形', 'ます', 'マス', 'マス']), ('。', ['記号', '句点', '*', '*', '*', '*', '。', '。', '。'])], Sad         0.110334\n",
      "Anx         0.114095\n",
      "Anger       0.199597\n",
      "Hate        0.145369\n",
      "Trustful    0.164159\n",
      "S           0.163723\n",
      "Happy       0.102724\n",
      "dtype: float64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def calc_jiwc(text: str, df_jiwc) -> np.ndarray:\n",
    "    doc = NLP(text.replace('一\\n\\n　', ''))\n",
    "    tokens = [token.lemma_ for sent in doc.sents for token in sent]\n",
    "\n",
    "    jiwc_words = set(\n",
    "        [token for token in tokens]\n",
    "    ) & set(df_jiwc.index)\n",
    "    jiwc_vals = df_jiwc.loc[jiwc_words].sum()\n",
    "    return tokens, np.divide(jiwc_vals, jiwc_vals.sum())\n",
    "    # Sad Anx Anger Hate Trustful S Happy\n",
    "    \n",
    "print(calc_jiwc(text, DF_jiwc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(['プログラミング', '言語', 'と', 'は', '異なる', '、', '自然', '言語', 'に', 'は', '言葉', 'の', '曖昧性', 'が', '存在', 'する', 'ます', '。'], Sad         0.111534\n",
      "Anx         0.115337\n",
      "Anger       0.199913\n",
      "Hate        0.139706\n",
      "Trustful    0.165945\n",
      "S           0.163723\n",
      "Happy       0.103842\n",
      "dtype: float64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "def measure_abst(text: str, awd) -> np.ndarray:\n",
    "    tokens = [\n",
    "        (n.surface, n.feature.split(\",\"))\n",
    "        for n in NMN.parse(text, as_nodes=True)\n",
    "        if not n.is_eos()\n",
    "    ]\n",
    "    \n",
    "    scores = [\n",
    "        float(awd.get(token[0] if token[1][6] == \"*\" else token[1][6], 0))\n",
    "        for token in tokens\n",
    "    ]\n",
    "\n",
    "    # top k=5 mean\n",
    "    return np.array([np.mean(sorted(scores, reverse=True)[:5]), max(scores)])\n",
    "print(measure_abst(text, AWD))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2.71 3.01]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def measure_abst(text: str, awd) -> np.ndarray:\n",
    "    doc = NLP(text.replace('一\\n\\n　', ''))\n",
    "    tokens = [token.lemma_ for sent in doc.sents for token in sent]\n",
    "    \n",
    "    scores = [\n",
    "        float(awd.get(token, 0))\n",
    "        for token in tokens\n",
    "    ]\n",
    "\n",
    "    # top k=5 mean\n",
    "    return np.array([np.mean(sorted(scores, reverse=True)[:5]), max(scores)])\n",
    "print(measure_abst(text, AWD))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2.596 2.75 ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "def detect_bunmatsu(text: str) -> float:\n",
    "    if \"\\r\" in text:\n",
    "        sents = text.split(\"\\r\\n\")\n",
    "    else:\n",
    "        sents = text.split(\"\\n\")\n",
    "    for sent in sents:\n",
    "        tokens = [\n",
    "            (n.surface, n.feature.split(\",\"))\n",
    "            for n in NM.parse(text, as_nodes=True)\n",
    "            if not n.is_eos()\n",
    "        ]\n",
    "    return tokens, tokens[-2][1][0]\n",
    "print(detect_bunmatsu(text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "([('プログラミング', ['名詞', 'サ変接続', '*', '*', '*', '*', 'プログラミング', 'プログラミング', 'プログラミング']), ('言語', ['名詞', '一般', '*', '*', '*', '*', '言語', 'ゲンゴ', 'ゲンゴ']), ('と', ['助詞', '格助詞', '一般', '*', '*', '*', 'と', 'ト', 'ト']), ('は', ['助詞', '係助詞', '*', '*', '*', '*', 'は', 'ハ', 'ワ']), ('異なり', ['動詞', '自立', '*', '*', '五段・ラ行', '連用形', '異なる', 'コトナリ', 'コトナリ']), ('、', ['記号', '読点', '*', '*', '*', '*', '、', '、', '、']), ('自然', ['名詞', '形容動詞語幹', '*', '*', '*', '*', '自然', 'シゼン', 'シゼン']), ('言語', ['名詞', '一般', '*', '*', '*', '*', '言語', 'ゲンゴ', 'ゲンゴ']), ('に', ['助詞', '格助詞', '一般', '*', '*', '*', 'に', 'ニ', 'ニ']), ('は', ['助詞', '係助詞', '*', '*', '*', '*', 'は', 'ハ', 'ワ']), ('言葉', ['名詞', '一般', '*', '*', '*', '*', '言葉', 'コトバ', 'コトバ']), ('の', ['助詞', '連体化', '*', '*', '*', '*', 'の', 'ノ', 'ノ']), ('曖昧', ['名詞', '形容動詞語幹', '*', '*', '*', '*', '曖昧', 'アイマイ', 'アイマイ']), ('性', ['名詞', '接尾', '一般', '*', '*', '*', '性', 'セイ', 'セイ']), ('が', ['助詞', '格助詞', '一般', '*', '*', '*', 'が', 'ガ', 'ガ']), ('存在', ['名詞', 'サ変接続', '*', '*', '*', '*', '存在', 'ソンザイ', 'ソンザイ']), ('し', ['動詞', '自立', '*', '*', 'サ変・スル', '連用形', 'する', 'シ', 'シ']), ('ます', ['助動詞', '*', '*', '*', '特殊・マス', '基本形', 'ます', 'マス', 'マス']), ('。', ['記号', '句点', '*', '*', '*', '*', '。', '。', '。'])], '助動詞')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "def detect_bunmatsu(text: str) -> float:\n",
    "    if \"\\r\" in text:\n",
    "        sents = text.split(\"\\r\\n\")\n",
    "    else:\n",
    "        sents = text.split(\"\\n\")\n",
    "\n",
    "    # 体言止め\n",
    "    taigen = 0\n",
    "    for sent in sents:\n",
    "        tokens = [\n",
    "            (n.surface, n.feature.split(\",\"))\n",
    "            for n in NM.parse(text, as_nodes=True)\n",
    "            if not n.is_eos()\n",
    "        ]\n",
    "        taigen += 1 if tokens[-2][1][0] == \"名詞\" else 0\n",
    "    ratio_taigen = np.divide(taigen, len(sents))\n",
    "\n",
    "    return tokens, ratio_taigen\n",
    "print(detect_bunmatsu(text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "([('プログラミング', ['名詞', 'サ変接続', '*', '*', '*', '*', 'プログラミング', 'プログラミング', 'プログラミング']), ('言語', ['名詞', '一般', '*', '*', '*', '*', '言語', 'ゲンゴ', 'ゲンゴ']), ('と', ['助詞', '格助詞', '一般', '*', '*', '*', 'と', 'ト', 'ト']), ('は', ['助詞', '係助詞', '*', '*', '*', '*', 'は', 'ハ', 'ワ']), ('異なり', ['動詞', '自立', '*', '*', '五段・ラ行', '連用形', '異なる', 'コトナリ', 'コトナリ']), ('、', ['記号', '読点', '*', '*', '*', '*', '、', '、', '、']), ('自然', ['名詞', '形容動詞語幹', '*', '*', '*', '*', '自然', 'シゼン', 'シゼン']), ('言語', ['名詞', '一般', '*', '*', '*', '*', '言語', 'ゲンゴ', 'ゲンゴ']), ('に', ['助詞', '格助詞', '一般', '*', '*', '*', 'に', 'ニ', 'ニ']), ('は', ['助詞', '係助詞', '*', '*', '*', '*', 'は', 'ハ', 'ワ']), ('言葉', ['名詞', '一般', '*', '*', '*', '*', '言葉', 'コトバ', 'コトバ']), ('の', ['助詞', '連体化', '*', '*', '*', '*', 'の', 'ノ', 'ノ']), ('曖昧', ['名詞', '形容動詞語幹', '*', '*', '*', '*', '曖昧', 'アイマイ', 'アイマイ']), ('性', ['名詞', '接尾', '一般', '*', '*', '*', '性', 'セイ', 'セイ']), ('が', ['助詞', '格助詞', '一般', '*', '*', '*', 'が', 'ガ', 'ガ']), ('存在', ['名詞', 'サ変接続', '*', '*', '*', '*', '存在', 'ソンザイ', 'ソンザイ']), ('し', ['動詞', '自立', '*', '*', 'サ変・スル', '連用形', 'する', 'シ', 'シ']), ('ます', ['助動詞', '*', '*', '*', '特殊・マス', '基本形', 'ます', 'マス', 'マス']), ('。', ['記号', '句点', '*', '*', '*', '*', '。', '。', '。'])], 0.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "def detect_bunmatsu(text: str) -> float:\n",
    "    doc = NLP(text.replace('一\\n\\n　', ''))\n",
    "    taigen = 0\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        tokens = []\n",
    "        for token in sent:\n",
    "            token_tag = re.split('[,-]', token.tag_)\n",
    "            tokens.append(token_tag)\n",
    "        taigen += 1 if tokens[-2][0]== \"名詞\" else 0\n",
    "    ratio_taigen = np.divide(taigen, len([doc.sents]))\n",
    "\n",
    "    return ratio_taigen\n",
    "# print(detect_bunmatsu(text))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "tai = '今日は晴天。'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "detect_bunmatsu(tai)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tokens_ginza(text: str) -> np.ndarray:\n",
    "    text = text.replace('一\\n\\n　', '')\n",
    "    doc = NLP(text)\n",
    "    tokens = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            token_tag = re.split('[,-]', token.tag_)\n",
    "            token_inflection = re.split('[,-]', ginza.inflection(token))\n",
    "            analysis = token_tag + token_inflection\n",
    "            analysis.append(token.lemma_)\n",
    "            analysis += re.split('[,-]', ginza.reading_form(token))\n",
    "            tuple_ = (token.orth_, analysis)\n",
    "            tokens.append(tuple_)\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}